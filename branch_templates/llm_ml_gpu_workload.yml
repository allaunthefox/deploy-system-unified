# =============================================================================
# Audit Event Identifier: DSU-PLY-100775
# Last Updated: 2026-02-28
# =============================================================================
---
# WARNING: THIS IS A REFERENCE TEMPLATE ONLY.
# DO NOT RUN THIS PLAYBOOK DIRECTLY FOR PRODUCTION DEPLOYMENTS.
# USE THE ROOT-LEVEL PLAYBOOKS (e.g., production_deploy.yml) INSTEAD.

# LLM/ML GPU Workload Deployment Template
# Optimized for Large Language Model and Machine Learning workloads
# Features: GPU resource allocation, high-performance settings, AI/ML frameworks support

- ansible.builtin.import_playbook: base_hardened.yml
- name: Deploy LLM/ML GPU Workload
  hosts: all
  become: true
  pre_tasks:
    - name: Validate System Identity (FQDN Enforcement)
      ansible.builtin.assert:
        that:
          - ansible_fqdn is defined
          - ansible_fqdn != "localhost"
          - ansible_fqdn != "localhost.localdomain"
        fail_msg: "System must have a valid FQDN configured for secure deployment."
    - name: Precreate essential directories (idempotent)
      ansible.builtin.file:
        path: "{{ item.path }}"
        state: directory
        mode: "{{ item.mode | default('0755') }}"
        owner: "{{ item.owner | default('root') }}"
        group: "{{ item.group | default('root') }}"
      loop:
        - { path: "/var/lib/deploy-system", mode: '0755', owner: 'root', group: 'root' }
        - { path: "/var/lib/deploy-system/checkpoints", mode: '0700', owner: 'root', group: 'root' }
        - { path: "/var/lib/deploy-system/backups", mode: '0700', owner: 'root', group: 'root' }
        - { path: "/var/lib/deploy-system/logs", mode: '0755', owner: 'root', group: 'root' }
        - { path: "/tmp/deploy-system", mode: '0755', owner: 'root', group: 'root' }
        - { path: "/etc/security", mode: '0755', owner: 'root', group: 'root' }
        - { path: "/etc/ssh", mode: '0755', owner: 'root', group: 'root' }
        - { path: "/etc/ufw", mode: '0755', owner: 'root', group: 'root' }
        - { path: "/etc/fail2ban", mode: '0755', owner: 'root', group: 'root' }
      become: true
      tags: [bootstrap, directories]
    - name: Set directory precreation flag (idempotent)
      ansible.builtin.set_fact:
        directories_precreated: true
    - name: Check if preflight validation already completed (idempotent)
      ansible.builtin.stat:
        path: /tmp/preflight_completed.flag
      register: preflight_flag
    - name: Include preflight checks if not already completed (idempotent)
      ansible.builtin.import_role:
        name: ops/preflight
      when: not preflight_flag.stat.exists
      tags: [preflight]
    - name: Create preflight completion flag (idempotent)
      ansible.builtin.file:
        path: /tmp/preflight_completed.flag
        state: touch
        mode: '0644'
      when: not preflight_flag.stat.exists

  roles:
    # Core system setup
    - core/bootstrap
    - core/identity
    - core/logging

    # Kubernetes node setup (for Kubernetes environments) - Skipped for test environment
    # - role: orchestration/k8s_node
    #   vars:
    #     k8s_gpu_device_plugins_enabled: true
    #     k8s_gpu_nvidia_device_plugin:
    #       enabled: true
    #       version: "v0.13.0"
    #       image: "nvcr.io/nvidia/k8s-device-plugin:v0.13.0"
    #     k8s_gpu_amd_device_plugin:
    #       enabled: false
    #       version: "v0.10.0"
    #       image: "rocm/k8s-device-plugin:v0.10.0"
    #     k8s_gpu_intel_device_plugin:
    #       enabled: false
    #       version: "v0.23.0"
    #       image: "intel/intel-gpu-plugin:v0.23.0"
    #     k8s_gpu_workload_scheduling:
    #       enabled: true
    #       scheduler_name: "gpu-scheduler"
    #       affinity:
    #         nodeAffinity:
    #           requiredDuringSchedulingIgnoredDuringExecution:
    #             nodeSelectorTerms:
    #               - matchExpressions:
    #                   - key: "nvidia.com/gpu.count"
    #                     operator: "Gt"
    #                     values: ["0"]

    # Hardware Drivers
    - role: hardware/firmware
    - role: hardware/gpu
      vars:
        gpu_stack_enable: true
        gpu_stack_vendor: "{{ containers_gpu_vendor | default('nvidia') }}"
        gpu_stack_enable_rdma: true # Critical for LLM training (NCCL)

    # GPU-specific configuration optimized for LLM/ML
    - role: containers/runtime
      vars:
        containers_enable_gpu_support: true
        containers_gpu_vendor: "nvidia"  # Set to "amd" or "intel" if needed
        containers_gpu_count: 1
        containers_gpu_slicing:
          strategy: "time-slicing"  # Use time-slicing for multi-workload isolation
          auto_strategy:
            bare_metal: "mig"
            virtual_host: "sriov"
            virtual_guest: "passthrough"
          time_slicing: { enabled: true, max_instances: 8 }  # Increased for more concurrent workloads
          mig: { enabled: false, profiles: ["1g.5gb"] }
          sriov: { enabled: false, vf_count: 4 }
          level_zero: { enabled: false, partitions: [] }
          oneapi: { enabled: false, toolkit: "base", components: ["compiler", "mpi", "tbb"] }
          passthrough: { devices: [] }

    # Quadlet configuration for container GPU support
    - role: containers/quadlets
      vars:
        quadlet_enable_gpu_support: true
        quadlet_gpu_vendor: "nvidia"
        quadlet_gpu_capabilities: []
        quadlet_container_gpu_config:
          anubis:
          caddy:

    # Container deployment with GPU support for LLM/ML
    - role: containers/anubis  # AI/ML platform
    - role: containers/caddy    # Web server

  post_tasks:
    - name: System optimization for LLM/ML workloads
      block:
        - name: Set swappiness to minimize swap usage
          ansible.builtin.lineinfile:
            path: /etc/sysctl.conf
            regexp: "^#?vm.swappiness ="
            line: "vm.swappiness = 5"
          become: true

        - name: Set huge pages for memory-intensive workloads
          ansible.builtin.lineinfile:
            path: /etc/sysctl.conf
            regexp: "^#?vm.nr_hugepages ="
            line: "vm.nr_hugepages = 1024"
          become: true

        - name: Increase file handle limits
          ansible.builtin.lineinfile:
            path: /etc/security/limits.conf
            regexp: "^#?\\* soft nofile"
            line: "* soft nofile 65536"
          become: true

        - name: Increase file handle hard limits
          ansible.builtin.lineinfile:
            path: /etc/security/limits.conf
            regexp: "^#?\\* hard nofile"
            line: "* hard nofile 65536"
          become: true

        - name: Apply sysctl changes
          ansible.builtin.command:
            cmd: sysctl -p
          become: true

    - name: Verify GPU functionality
      ansible.builtin.command:
        cmd: nvidia-smi
      register: nvidia_smi_output
      changed_when: false
      failed_when: false
      become: false
      when: containers_gpu_vendor == "nvidia"

    - name: Verify Kubernetes GPU device plugin status (if Kubernetes is enabled)
      ansible.builtin.command:
        cmd: kubectl get daemonsets -n kube-system
      register: kube_device_plugins
      changed_when: false
      failed_when: false
      become: false
      when: ansible_facts['kernel'] is search('k8s')

    - name: Display LLM/ML GPU workload information
      ansible.builtin.debug:
        msg: |
          LLM/ML GPU Workload Deployment Complete
          Vendor: {{ containers_gpu_vendor | upper }}
          Slicing Strategy: {{ containers_gpu_slicing.strategy }}
          Time Slicing Instances: {{ containers_gpu_slicing.time_slicing.max_instances }}
          Container Runtime: Podman with GPU support enabled
          Kubernetes GPU Support: {{ k8s_gpu_device_plugins_enabled if 'k8s_gpu_device_plugins_enabled' in vars else 'Disabled' }}
          Kubernetes Device Plugins: {{ kube_device_plugins.stdout | default('N/A') }}
          GPU Resources: {{ nvidia_smi_output.stdout | default('No GPU device detected') }}
          System Optimizations: Applied (swappiness=5, huge pages=1024, file limits=65536)
          Container Deployment: Anubis (AI/ML) and Caddy (Web) containers with GPU support
